{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epanechnikov_kernel(z):\n",
    "    return (3 / 4) * (1 - z ** 2) * (np.abs(z) < 1)\n",
    "\n",
    "def gaussian_kernel(z):\n",
    "    return (2 * math.pi) ** (-1 / 2) * np.exp(z ** 2 / 2)\n",
    "\n",
    "def triweight_kernel(z):\n",
    "    return (35 / 32) * (1 - z ** 2) ** 3 * (np.abs(z) < 1)\n",
    "\n",
    "def plug_in_h(X, delta):\n",
    "    N = X.shape[0]\n",
    "    stdev = np.std(X)\n",
    "    \n",
    "    return 1.3643 * delta * (N ** (-1 / 5)) * stdev\n",
    "\n",
    "def silverman_h(X, delta):\n",
    "    N = X.shape[0]\n",
    "    stdev = np.std(X)\n",
    "    iqr = np.percentile(X, 75, interpolation='midpoint') - np.percentile(X, 25, interpolation='midpoint')\n",
    "    \n",
    "    return 1.3643 * delta * (N ** (-1 / 5)) * np.min([stdev, iqr / 1.349])\n",
    "\n",
    "def cross_validation_h(Y, X, kernel):\n",
    "    N = X.shape[0]\n",
    "    bandwidth_values = np.arange(0.15, 1, 0.05)\n",
    "    \n",
    "    optimal_h = 10000\n",
    "    CV = 10000\n",
    "    \n",
    "    if kernel == 'epanechnikov':\n",
    "        k = epanechnikov_kernel\n",
    "    elif kernel == 'gaussian':\n",
    "        k = gaussian_kernel\n",
    "    else:\n",
    "        k = triweight_kernel\n",
    "        \n",
    "    iota = np.ones(N)\n",
    "    differ = np.zeros(N)\n",
    "    for bandwidth in bandwidth_values:\n",
    "        for i in range(N):\n",
    "            weights = np.zeros(N)\n",
    "\n",
    "            X_0 = X[i]\n",
    "            Z = (iota * X_0 - X) / bandwidth\n",
    "            KX = k(Z)\n",
    "\n",
    "            YKX = Y * KX\n",
    "            m_regress = np.mean(YKX) / np.mean(KX)\n",
    "\n",
    "            for j in range(N):\n",
    "                Z_nom = (X_0 - X[j]) / bandwidth\n",
    "                nominator = (1 / (N * bandwidth)) * k(Z_nom)\n",
    "\n",
    "                Z_denom = (iota * X_0 - X) / bandwidth\n",
    "                denominator = (1 / (N * bandwidth)) * np.sum(k(Z_denom))\n",
    "\n",
    "                weights[j] = nominator / denominator\n",
    "\n",
    "            differ[i] = (Y[i] - m_regress) / (1 - (weights[i] / np.sum(weights)))\n",
    "\n",
    "        lower_X = np.percentile(X, 5, interpolation='midpoint')\n",
    "        upper_X = np.percentile(X, 95, interpolation='midpoint')\n",
    "\n",
    "        pi = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            pi[i] = int(((X[i] < upper_X) & (X[i] > lower_X)))\n",
    "\n",
    "        CV_val = np.sum(differ ** 2 * pi)\n",
    "        \n",
    "        if CV_val < CV:\n",
    "            CV = CV_val\n",
    "            optimal_h = bandwidth\n",
    "        \n",
    "        \n",
    "        \n",
    "    return optimal_h         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npregress(Y, X, kernel, bandwidth, bandwidth_optimal=None, Xmidpoints=None, nrbins = 100):\n",
    "    \n",
    "    nrows, ncols = X.shape\n",
    "    \n",
    "    YX = np.concatenate((Y, X), axis=1)\n",
    "    YX = YX[YX[:,1].argsort()]\n",
    "    \n",
    "    Y = YX[:, 0]\n",
    "    X = YX[:, 1]\n",
    "    \n",
    "    meanX = np.mean(X)\n",
    "    stdX = np.std(X)\n",
    "    iota = np.ones([nrows, 1])\n",
    "    \n",
    "    if Xmidpoints == None:\n",
    "        nrbins = nrbins\n",
    "        \n",
    "        lowerX = X[int(np.floor(0.01 * nrows))]\n",
    "        upperX = X[int(np.floor(0.99 * nrows - 1))]\n",
    "        \n",
    "        Xmidpoints_used = np.reshape(np.linspace(lowerX, upperX, nrbins), newshape=(nrbins, 1))\n",
    "        \n",
    "    else:\n",
    "        nrbins, ncbins = Xmidpoints.size\n",
    "        Xmidpoints_used = Xmidpoints\n",
    "        \n",
    "    firstb = Xmidpoints_used[0, :]\n",
    "    lastb = Xmidpoints_used[nrbins - 1, :]\n",
    "    binsize = ((lastb - firstb) / nrbins)\n",
    "    \n",
    "    if kernel == 'epanechnikov':\n",
    "        k = epanechnikov_kernel\n",
    "        delta = 1.7188\n",
    "    elif kernel == 'gaussian':\n",
    "        k = gaussian_kernel\n",
    "        delta = 0.7764\n",
    "    else:\n",
    "        k = triweight_kernel\n",
    "        delta = 2.3122\n",
    "    \n",
    "    if bandwidth_optimal == None:\n",
    "        bandwidth_used = bandwidth\n",
    "    elif bandwidth_optimal == 'plug_in':\n",
    "        bandwidth_used = plug_in_h(X, delta)\n",
    "    elif bandwidth_optimal == 'silverman':\n",
    "        bandwidth_used = silverman_h(X, delta)\n",
    "    else:\n",
    "        bandwidth_used = cross_validation_h(Y, X, kernel)\n",
    "        \n",
    "    m_regress = np.zeros(nrbins)\n",
    "    \n",
    "    for j in range(nrbins):\n",
    "        Xb = Xmidpoints_used[j, 0]\n",
    "        Z = (iota * Xb - X) / bandwidth_used\n",
    "        KX = k(Z)\n",
    "        \n",
    "        YKX = Y * KX\n",
    "        m_regress[j] = np.mean(YKX) / np.mean(KX)\n",
    "\n",
    "        \n",
    "    return Xmidpoints_used, m_regress, bandwidth_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normpdf(x):\n",
    "    pi = 3.1415926\n",
    "    denom = (2*pi*np.var(x))**.5\n",
    "    num = np.exp(-(x-np.mean(x))**2/(2*np.var(x)))\n",
    "    return num/denom\n",
    "\n",
    "def generate_data(N):\n",
    "    \n",
    "    X = np.random.uniform(low=0, high=30, size=(N, 1))\n",
    "    u = np.random.normal(loc=0, scale=1, size=(N, 1))\n",
    "    X = np.sort(X)\n",
    "    y = (np.sin(X) + 2) * X + u\n",
    "    \n",
    "    yX = np.concatenate((y, X), axis=1) \n",
    "    \n",
    "    yX = yX[yX[:, 1].argsort()]\n",
    "    \n",
    "    return yX "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_weighted_OLS(dataset, index):\n",
    "    \n",
    "    ##### INPUT\n",
    "    \n",
    "    ### data \n",
    "    # is a Nxk dimensional array, having y in the first column and X in all following columns\n",
    "    \n",
    "    ### index\n",
    "    # index of an observation for which the weighted OLS has to be calculated\n",
    "    \n",
    "    ##### OUTPUT\n",
    "    # 1x1 array (or float) containing local weighted average of the observation \n",
    "    \n",
    "    y = dataset[:, 0]\n",
    "    X = dataset[:, 1]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    x_0 = X[index]\n",
    "    \n",
    "    var_X = np.sum((X - np.mean(X)) ** 2)\n",
    "    peacewise_var_X = X - np.mean(X)\n",
    "    \n",
    "    OLS_est = np.sum(((1 / N) + (x_0 - np.mean(X)) * peacewise_var_X / var_X ) * y)\n",
    "    \n",
    "    return OLS_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS_est = np.zeros([data.shape[0], 2])\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    OLS_est[i, 0] = local_weighted_OLS(data, i)\n",
    "    OLS_est[i, 1] = data[i, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:, 1], data[:, 0])\n",
    "plt.plot(OLS_est[:, 1], OLS_est[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $k$ nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_sinx_x(N):\n",
    "    \n",
    "    X = np.random.uniform(low=0, high=30, size=(N, 1))\n",
    "    u = np.random.normal(loc=0, scale=10, size=(N, 1))\n",
    "    y = (np.sin(X) + 2) * X + u\n",
    "    \n",
    "    yX = np.concatenate((y, X), axis=1) \n",
    "    \n",
    "    yX = yX[yX[:, 1].argsort()]\n",
    "    \n",
    "    return yX\n",
    "\n",
    "def generate_data_expx_x(N):\n",
    "    \n",
    "    X = np.random.uniform(low=0, high=8, size=(N, 1))\n",
    "    u = np.random.normal(loc=0, scale=10, size=(N, 1))\n",
    "    y = (np.exp(X) + 2) * X + u\n",
    "    \n",
    "    yX = np.concatenate((y, X), axis=1) \n",
    "    \n",
    "    yX = yX[yX[:, 1].argsort()]\n",
    "    \n",
    "    return yX\n",
    "\n",
    "def knn_mean (response, no, step):\n",
    "    if no - step >= 0 & no + step <= response.shape[0]:\n",
    "        knn_mean_i = np.mean(response[(no - step) : (no + step + 1)])\n",
    "        return(knn_mean_i)\n",
    "    else:\n",
    "        return(knn_mean(response, no, step-1))\n",
    "            \n",
    "\n",
    "def knn_regr (dataset, k):\n",
    "    #print('Be sure that Y is the first column and X is the second column!')\n",
    "    N = dataset.shape[0]\n",
    "    target = np.zeros(N)\n",
    "    data = dataset[dataset[:, 1].argsort()]\n",
    "    Y = dataset[:, 0]\n",
    "    X = dataset[:, 1]\n",
    "                    \n",
    "    for i in range(N):\n",
    "        target[i] = knn_mean(Y, i, k)\n",
    "        \n",
    "    return(target)\n",
    "\n",
    "def MSE (target, predicted):\n",
    "    return(np.sum((target-predicted) ** 2))\n",
    "\n",
    "def discr_smoothness(series):\n",
    "    sum_of_jumps = 0\n",
    "    for i in range(series.shape[0]-1):\n",
    "        sum_of_jumps += np.abs(series[i+1] - series[i])\n",
    "        \n",
    "    return(sum_of_jumps)    \n",
    "    \n",
    "def knn_regr_komplit (dataset, init_k, lam):\n",
    "    data = dataset[dataset[:, 1].argsort()]\n",
    "    predicted = knn_regr(data, init_k)\n",
    "    ms_error = MSE(predicted, data[:, 0])    \n",
    "    jumps = discr_smoothness(predicted)\n",
    "    penal = np.abs(ms_error + lam * jumps)\n",
    "           \n",
    "    return(ms_error, discr_smoothness(predicted), penal, predicted)\n",
    "\n",
    "def optimal_k (dataset, init_k, lam):\n",
    "    data = dataset[dataset[:, 1].argsort()]\n",
    "    a, b, c, d = knn_regr_komplit(data, init_k, lam)\n",
    "    predicted_init = d\n",
    "    ms_error_init = a  \n",
    "    jumps_init = b\n",
    "    penal_best = c\n",
    "    best_k = init_k       \n",
    "    \n",
    "    for k in np.arange(6, 153, 3):\n",
    "        penal_new = knn_regr_komplit(data, k, lam)[2]\n",
    "        if penal_new < penal_best:\n",
    "            best_k = k\n",
    "        \n",
    "    return(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data_sinx_x(1000)\n",
    "lam = 40\n",
    "opt_k = optimal_k(data, 20, lam)        \n",
    "a, b, c, d = knn_regr_komplit(data, opt_k, lam)\n",
    "plt.scatter(data[:, 1], data[:, 0], alpha=0.4)\n",
    "plt.plot(data[:, 1], (np.sin(data[:, 1]) + 2) * data[:, 1], c = 'black')\n",
    "plt.plot(data[:, 1], d, c = 'red')\n",
    "plt.show()\n",
    "\n",
    "print('MSE:', a)\n",
    "print('Jumps:', b)\n",
    "print('Penal:', c)\n",
    "print('For lambda =', lam, ' the optimal k is:', opt_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data_expx_x(1000)\n",
    "lam = 40\n",
    "opt_k = optimal_k(data, 20, lam)        \n",
    "a, b, c, d = knn_regr_komplit(data, opt_k, lam)\n",
    "plt.scatter(data[:, 1], data[:, 0], alpha=0.4)\n",
    "plt.plot(data[:, 1], (np.exp(data[:, 1]) + 2) * data[:, 1], c = 'black')\n",
    "plt.plot(data[:, 1], d, c = 'red')\n",
    "plt.show()\n",
    "\n",
    "print('MSE:', a)\n",
    "print('Jumps:', b)\n",
    "print('Penal:', c)\n",
    "print('For lambda =', lam, ' the optimal k is:', opt_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epanechnikov kernel for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(data[:, 0], (data[:, 0].shape[0], 1))\n",
    "X = np.reshape(data[:, 1], (data[:, 1].shape[0], 1))\n",
    "\n",
    "midpoints, fitted, band = npregress(Y=y, X=X, kernel='epanechnikov', \n",
    "                                    bandwidth=0.2, bandwidth_optimal='cross_validation',\n",
    "                                    nrbins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, alpha=0.1)\n",
    "plt.plot(midpoints, fitted, c='orange')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
